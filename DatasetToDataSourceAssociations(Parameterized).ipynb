{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baaad5e5",
   "metadata": {},
   "source": [
    "#Adding a PowerBI DataSource ID on to a PowerBI DataSet table via PowerBI REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read JSON on local\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "pbi_dataset_df = pd.read_json(r'C:\\Users\\aserlovsky\\Downloads\\{filename}.json')\n",
    "print(pbi_dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten JSON File function\n",
    "\n",
    "def flatten_nested_json_df(df):\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    \n",
    "    print(f\"original shape: {df.shape}\")\n",
    "    print(f\"original columns: {df.columns}\")\n",
    "    \n",
    "    \n",
    "    # search for columns to explode/flatten\n",
    "    s = (df.applymap(type) == list).all()\n",
    "    list_columns = s[s].index.tolist()\n",
    "    \n",
    "    s = (df.applymap(type) == dict).all()\n",
    "    dict_columns = s[s].index.tolist()\n",
    "    \n",
    "    print(f\"lists: {list_columns}, dicts: {dict_columns}\")\n",
    "    while len(list_columns) > 0 or len(dict_columns) > 0:\n",
    "        new_columns = []\n",
    "        \n",
    "        for col in dict_columns:\n",
    "            print(f\"flattening: {col}\")\n",
    "            # explode dictionaries horizontally, adding new columns\n",
    "            horiz_exploded = pd.json_normalize(df[col]).add_prefix(f'{col}.')\n",
    "            horiz_exploded.index = df.index\n",
    "            df = pd.concat([df, horiz_exploded], axis=1).drop(columns=[col])\n",
    "            new_columns.extend(horiz_exploded.columns) # inplace\n",
    "        \n",
    "        for col in list_columns:\n",
    "            print(f\"exploding: {col}\")\n",
    "            # explode lists vertically, adding new columns\n",
    "            df = df.drop(columns=[col]).join(df[col].explode().to_frame())\n",
    "            # Prevent combinatorial explosion when multiple\n",
    "            # cols have lists or lists of lists\n",
    "            df = df.reset_index(drop=True)\n",
    "            new_columns.append(col)\n",
    "        \n",
    "        # check if there are still dict o list fields to flatten\n",
    "        s = (df[new_columns].applymap(type) == list).all()\n",
    "        list_columns = s[s].index.tolist()\n",
    "\n",
    "        s = (df[new_columns].applymap(type) == dict).all()\n",
    "        dict_columns = s[s].index.tolist()\n",
    "        \n",
    "        print(f\"lists: {list_columns}, dicts: {dict_columns}\")\n",
    "        \n",
    "    print(f\"final shape: {df.shape}\")\n",
    "    print(f\"final columns: {df.columns}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fba2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call flatten function\n",
    "flattened_dataset_df = flatten_nested_json_df(pbi_dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate per each datasetid per each API call\n",
    "import requests\n",
    "import json\n",
    "\n",
    "list_of_datasourcesbydataset = []\n",
    "count = 0\n",
    "\n",
    "dataset_ids = flattened_dataset_df['value.id']\n",
    "for dataset_id in dataset_ids:\n",
    "    url = f\"https://api.powerbi.com/v1.0/myorg/admin/datasets/{dataset_id}/datasources\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "      'Authorization': 'Bearer {BearerToken}'   }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    response_dict = json.loads(response.text)\n",
    "    response_dict['datasetid'] = dataset_id\n",
    "    list_of_datasourcesbydataset.append(response_dict)\n",
    "    count += 1\n",
    "    print(count, dataset_id)\n",
    "    \n",
    "print(list_of_datasourcesbydataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save File\n",
    "import os\n",
    "def write_json(new_data, filename=None):\n",
    "    desired_dir = \"C:\\\\Users\\\\aserlovsky\\\\\"\n",
    "    full_path = os.path.join(desired_dir, filename)\n",
    "    with open(full_path, 'w') as f:\n",
    "        json_string=json.dumps(new_data)\n",
    "        f.write(json_string)\n",
    "        \n",
    "write_json(list_of_datasourcesbydataset, filename=\"datasources.json\")\n",
    "# json_string = json.dumps(list_of_datasourcesbydataset)\n",
    "# print(json_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd2f3d",
   "metadata": {},
   "source": [
    "#References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41415c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check current working directory (cwd)\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POST Access Token\n",
    "import requests\n",
    "\n",
    "url = \"https://login.microsoftonline.com/{tenantID}/oauth2/token\"\n",
    "\n",
    "payload = 'tenant_id={tenantID}&grant_type=client_credentials&client_id={ClientID}&client_secret={ClientSecret}&resource=https%3A%2F%2Fanalysis.windows.net%2Fpowerbi%2Fapi&scope=api%3A%2F%2F{ClientID}%2FPBIAPI'\n",
    "headers = {\n",
    "  'Content-Type': 'application/x-www-form-urlencoded',\n",
    "  'Authorization': 'Bearer {BearerToken}\n",
    "  'Cookie': {Cookie} #Optional\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API Call to get dataset\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = f\"https://api.powerbi.com/v1.0/myorg/admin/datasets\"\n",
    "\n",
    "payload = {}\n",
    "headers = {\n",
    "  'Authorization': 'Bearer {BearerToken}}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "response_dict = json.loads(response.text)\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76eb5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API Call to get datasource\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = f\"https://api.powerbi.com/v1.0/myorg/admin/datasets/{DatasetID}/datasources\"\n",
    "\n",
    "payload = {}\n",
    "headers = {\n",
    "  'Authorization': 'Bearer {BearerToken}}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "response_dict = json.loads(response.text)\n",
    "print(response_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
